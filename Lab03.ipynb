{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8PsD0Lw0g_mL"
   },
   "source": [
    "# Lab 3: Minimizing Cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPg1vs52g_mQ"
   },
   "source": [
    "Author: Seungjae Lee (이승재)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A1VVTA6Xg_mR"
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    We use elemental PyTorch to implement linear regression here. However, in most actual applications, abstractions such as <code>nn.Module</code> or <code>nn.Linear</code> are used.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSf_efZxg_mS"
   },
   "source": [
    "## Theoretical Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hOXEnwKg_mS"
   },
   "source": [
    "$$ H(x) = Wx $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "juqvz7hSg_mT"
   },
   "source": [
    "$$ cost(W) = \\frac{1}{m} \\sum^m_{i=1} \\left( Wx^{(i)} - y^{(i)} \\right)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VFNYs7EBg_mU"
   },
   "source": [
    " - $H(x)$: 주어진 $x$ 값에 대해 예측을 어떻게 할 것인가\n",
    " - $cost(W)$: $H(x)$ 가 $y$ 를 얼마나 잘 예측했는가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPBuQBPJg_mV"
   },
   "source": [
    "Note that it is simplified, without the bias $b$ added to $H(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9OqqeHgg_mX"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_EzXq4xBg_mY"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "KCD_9kMcg_ma",
    "outputId": "0c5c2cf3-04bf-4039-d356-e240a5a1aa97"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1756de68618>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For reproducibility\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DX-yid2yg_mb"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMDL0D3Kg_mc"
   },
   "source": [
    "We will use fake data for this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SWh1HBeqg_mc"
   },
   "source": [
    "기본적으로 PyTorch는 NCHW 형태이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "GycghV9Og_mc"
   },
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sdyERsQEg_md",
    "outputId": "4a3e17ad-eb2b-42df-b819-19b8b0407168"
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "plt.scatter(x_train, y_train)\n",
    "# Best-fit line\n",
    "xs = np.linspace(1, 3, 1000)\n",
    "plt.plot(xs, xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jblHIh7Zg_md"
   },
   "source": [
    "## Cost by W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6GXN7TCg_md"
   },
   "source": [
    "$$ H(x) = Wx $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "nQ44HY6hg_me"
   },
   "outputs": [],
   "source": [
    "W_l = np.linspace(-5, 7, 1000)\n",
    "cost_l = []\n",
    "for W in W_l:\n",
    "    hypothesis = W * x_train\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "\n",
    "    cost_l.append(cost.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XY-t2-wtg_me",
    "outputId": "915cc4cd-4d78-4eca-97c2-0b57b8e97e1d"
   },
   "outputs": [],
   "source": [
    "plt.plot(W_l, cost_l)\n",
    "plt.xlabel('$W$')\n",
    "plt.ylabel('Cost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Caylv3xg_mf"
   },
   "source": [
    "## Gradient Descent by Hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5pRl-n4rg_mf"
   },
   "outputs": [],
   "source": [
    "W = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCAMFpKjg_mf"
   },
   "source": [
    "$$ cost(W) = \\frac{1}{m} \\sum^m_{i=1} \\left( Wx^{(i)} - y^{(i)} \\right)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ZpkeAeqg_mf"
   },
   "source": [
    "$$ \\nabla W = \\frac{\\partial cost}{\\partial W} = \\frac{2}{m} \\sum^m_{i=1} \\left( Wx^{(i)} - y^{(i)} \\right)x^{(i)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Vn8RhN7ng_mg",
    "outputId": "c726bdbd-42f7-4ee5-db1a-5a17b631d4b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-14.)\n"
     ]
    }
   ],
   "source": [
    "gradient = torch.sum((W * x_train - y_train) * x_train)\n",
    "print(gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xv6NEYi_g_mg"
   },
   "source": [
    "$$ W := W - \\alpha \\nabla W $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "f4zRrlSJg_mg",
    "outputId": "7f45ff63-b630-4c0e-fa1e-131c1b1db52a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4000)\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1\n",
    "W -= lr * gradient\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s6lQjuziKJBh"
   },
   "source": [
    "Q1. 경사하강법(Gradient Descent) W:=W−α∇W에서 -를 쓰는 이유는?\n",
    "\n",
    "=> 기울기가 음수인 경우에는 +를 하고 기울기가 양수인 경우에는 -를 사용한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qi8YmCKTg_mg"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCwk2_NTjr6p"
   },
   "source": [
    "위의 내용을 참고해 코드를 완성시키세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "lYYQHd6sg_mh",
    "outputId": "8b250d4e-b233-4f6c-8b6f-c1d854e27bae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/10 W: 0.000, Cost: 4.666667\n",
      "Epoch    1/10 W: 1.400, Cost: 0.746666\n",
      "Epoch    2/10 W: 0.840, Cost: 0.119467\n",
      "Epoch    3/10 W: 1.064, Cost: 0.019115\n",
      "Epoch    4/10 W: 0.974, Cost: 0.003058\n",
      "Epoch    5/10 W: 1.010, Cost: 0.000489\n",
      "Epoch    6/10 W: 0.996, Cost: 0.000078\n",
      "Epoch    7/10 W: 1.002, Cost: 0.000013\n",
      "Epoch    8/10 W: 0.999, Cost: 0.000002\n",
      "Epoch    9/10 W: 1.000, Cost: 0.000000\n",
      "Epoch   10/10 W: 1.000, Cost: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\n",
    "# 모델 초기화\n",
    "W = torch.zeros(1) \n",
    "\n",
    "#이 부분을 채워넣으세요.# #가중치 W를 0으로 초기화하세요. \n",
    "#Gradient Descent by Hand이기 때문에 학습을 통해 값이 변경되는 것이 아닙니다!\n",
    "# learning rate 설정\n",
    "lr = 0.1\n",
    "cost_l = []\n",
    "\n",
    "nb_epochs = 10\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    hypothesis = W * x_train   #이 부분을 채워넣으세요.# # H(x)=Wx를 이용하세요.\n",
    "    \n",
    "    # cost gradient 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)  #이 부분을 채워넣으세요.# #파이토치 코드 상으로 선형 회귀의 비용 함수에 해당되는 평균 제곱 오차를 선언하세요.\n",
    "    \n",
    "    gradient = torch.sum((W * x_train - y_train) * x_train)  #이 부분을 채워넣으세요.# #Gradient Descent by Hand를 이용하세요.\n",
    "\n",
    "    print('Epoch {:4d}/{} W: {:.3f}, Cost: {:.6f}'.format(\n",
    "        epoch, nb_epochs, W.item(), cost.item()\n",
    "    ))\n",
    "\n",
    "    # cost gradient로 H(x) 개선\n",
    "    \n",
    "    W -= lr * gradient #이 부분을 채워넣으세요.# #W:=W−α∇W을 이용하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9iaJEB1Eg_mh"
   },
   "source": [
    "## Training with `optim`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLki9JjIkyEL"
   },
   "source": [
    "위의 내용을 참고해 코드를 완성시키세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ZVBbc9t9g_mh",
    "outputId": "8f8397eb-5c46-4232-8f89-045dad4851f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/10 W: 0.000 Cost: 4.666667\n",
      "Epoch    1/10 W: 1.400 Cost: 0.746667\n",
      "Epoch    2/10 W: 0.840 Cost: 0.119467\n",
      "Epoch    3/10 W: 1.064 Cost: 0.019115\n",
      "Epoch    4/10 W: 0.974 Cost: 0.003058\n",
      "Epoch    5/10 W: 1.010 Cost: 0.000489\n",
      "Epoch    6/10 W: 0.996 Cost: 0.000078\n",
      "Epoch    7/10 W: 1.002 Cost: 0.000013\n",
      "Epoch    8/10 W: 0.999 Cost: 0.000002\n",
      "Epoch    9/10 W: 1.000 Cost: 0.000000\n",
      "Epoch   10/10 W: 1.000 Cost: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\n",
    "# 모델 초기화\n",
    "W = torch.zeros(1,requires_grad = True)  #이 부분을 채워넣으세요.# #가중치 W를 0으로 초기화하고 학습을 통해 값이 변경될 수 있도록 하세요.\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W], lr = 0.15)\n",
    "#이 부분을 채워넣으세요.# #SGD optimizer를 사용하고 learning rate는 0.15로 적용하세요.\n",
    "\n",
    "nb_epochs = 10\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    # H(x) 계산\n",
    "    hypothesis = W * x_train \n",
    "    #이 부분을 채워넣으세요.# # H(x)=Wx를 이용하세요.\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) **2 )\n",
    "    \n",
    "    #이 부분을 채워넣으세요.# #파이토치 코드 상으로 선형 회귀의 비용 함수에 해당되는 평균 제곱 오차를 선언하세요.\n",
    "\n",
    "    print('Epoch {:4d}/{} W: {:.3f} Cost: {:.6f}'.format(\n",
    "        epoch, nb_epochs, W.item(), cost.item()\n",
    "    ))\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    #이 부분을 채워넣으세요.#  # gradient를 0으로 초기화\n",
    "    #이 부분을 채워넣으세요.#  # 비용 함수를 미분하여 gradient 계산\n",
    "    #이 부분을 채워넣으세요.#  # W와 b를 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab03.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
